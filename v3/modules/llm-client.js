// LLM API í´ë¼ì´ì–¸íŠ¸ ëª¨ë“ˆ
// OpenAI, Anthropic ë“± ë‹¤ì–‘í•œ LLM ì œê³µìë¥¼ ì§€ì›

class LLMClient {
    constructor(config) {
        // ì§€ì›í•˜ëŠ” ì œê³µì: 'openai' | 'anthropic' | 'gemini' | 'groq' | 'huggingface' | 'openrouter' | 'ollama' | 'custom'
        this.provider = config?.provider || 'openai';
        this.apiKey = config?.apiKey || '';
        this.model = config?.model || 'gpt-4o-mini';
        this.baseURL = config?.baseURL || null; // ì»¤ìŠ¤í…€ ì—”ë“œí¬ì¸íŠ¸ìš©
        this.maxTokens = config?.maxTokens || 2000;
        this.temperature = config?.temperature || 0.7;
    }

    /**
     * LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„± (RAG íŒ¨í„´)
     * @param {string} question - ì‚¬ìš©ì ì§ˆë¬¸
     * @param {Array} contextData - ê²€ìƒ‰ëœ ë°ì´í„° (ì»¨í…ìŠ¤íŠ¸)
     * @param {Object} metadata - ì¶”ê°€ ë©”íƒ€ë°ì´í„° (íƒ€ê²Ÿ ì»¬ëŸ¼, í•„í„° ì¡°ê±´ ë“±)
     * @returns {Promise<string>} LLMì´ ìƒì„±í•œ ë‹µë³€
     */
    async generateAnswer(question, contextData = [], metadata = {}) {
        if (!this.apiKey) {
            throw new Error('LLM API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. config.jsì—ì„œ LLM_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.');
        }

        // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        const systemPrompt = this.buildSystemPrompt(metadata);
        
        // ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        const userPrompt = this.buildUserPrompt(question, contextData, metadata);

        try {
            let response;
            
            switch (this.provider) {
                case 'openai':
                    response = await this.callOpenAI(systemPrompt, userPrompt);
                    break;
                case 'anthropic':
                    response = await this.callAnthropic(systemPrompt, userPrompt);
                    break;
                case 'gemini':
                    response = await this.callGemini(systemPrompt, userPrompt);
                    break;
                case 'groq':
                    response = await this.callGroq(systemPrompt, userPrompt);
                    break;
                case 'huggingface':
                    response = await this.callHuggingFace(systemPrompt, userPrompt);
                    break;
                case 'openrouter':
                    response = await this.callOpenRouter(systemPrompt, userPrompt);
                    break;
                case 'ollama':
                    response = await this.callOllama(systemPrompt, userPrompt);
                    break;
                case 'custom':
                    response = await this.callCustomAPI(systemPrompt, userPrompt);
                    break;
                default:
                    throw new Error(`ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: ${this.provider}`);
            }

            return response;
        } catch (error) {
            console.error('LLM API í˜¸ì¶œ ì˜¤ë¥˜:', error);
            throw new Error(`LLM ë‹µë³€ ìƒì„± ì‹¤íŒ¨: ${error.message}`);
        }
    }

    /**
     * ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
     */
    buildSystemPrompt(metadata) {
        return `ë‹¹ì‹ ì€ í™˜ê²½ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ ì±—ë´‡ì…ë‹ˆë‹¤. ìˆ˜ì§ˆ, ë…¹ì¡°, ìˆ˜ë¬¸, ê¸°ìƒ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ì—­í• :
1. ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±
2. ìˆ˜ì§ˆ ë“±ê¸‰, ì¡°ë¥˜ ê²½ë³´ ë‹¨ê³„ ë“± ì „ë¬¸ ì§€ì‹ í™œìš©
3. ê²½ê³ ê°€ ìˆì„ ë•ŒëŠ” ëŒ€ì‘ ë©”ë‰´ì–¼ì˜ êµ¬ì²´ì ì¸ ì¡°ì¹˜ ë°©ë²•ì„ ì œì‹œ
4. ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ ê¶ê¸ˆí•´í•  ë§Œí•œ ì •ë³´ë¥¼ ëŠ¥ë™ì ìœ¼ë¡œ ì œì•ˆ
5. ë°ì´í„°ê°€ ì—†ì„ ë•ŒëŠ” ì¹œì ˆí•˜ê²Œ ì•ˆë‚´

ì¤‘ìš”: ê²½ê³ ê°€ ìˆê³  ëŒ€ì‘ ë©”ë‰´ì–¼ì´ ì œê³µë˜ë©´, ë‹¨ìˆœíˆ "ë©”ë‰´ì–¼ì„ ì°¸ê³ í•˜ì„¸ìš”"ë¼ê³  ë§í•˜ì§€ ë§ê³ , ë©”ë‰´ì–¼ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ì¡°ì¹˜ ë°©ë²•ì„ ì§ì ‘ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ë‹µë³€ ìŠ¤íƒ€ì¼:
- í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€
- ì „ë¬¸ ìš©ì–´ëŠ” ê°„ë‹¨íˆ ì„¤ëª…
- ìˆ«ìì™€ ë‹¨ìœ„ë¥¼ ëª…í™•íˆ í‘œì‹œ
- ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ
- ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê°ê´€ì ì¸ ë¶„ì„ ì œê³µ

ìˆ˜ì§ˆ ë“±ê¸‰ ê¸°ì¤€:
- Ië“±ê¸‰: ë§¤ìš° ì¢‹ìŒ (pH 6.5-8.5, BOD â‰¤1.0, T-N â‰¤0.2, T-P â‰¤0.02)
- IIë“±ê¸‰: ì¢‹ìŒ (pH 6.0-9.0, BOD â‰¤2.0, T-N â‰¤0.3, T-P â‰¤0.04)
- IIIë“±ê¸‰: ë³´í†µ (pH 5.5-9.5, BOD â‰¤3.0, T-N â‰¤0.5, T-P â‰¤0.1)
- IVë“±ê¸‰: ë‚˜ì¨ (pH 5.0-10.0, BOD â‰¤5.0, T-N â‰¤1.0, T-P â‰¤0.2)
- Vë“±ê¸‰: ë§¤ìš° ë‚˜ì¨

ì¡°ë¥˜ ê²½ë³´ ë‹¨ê³„:
- ì •ìƒ: FAI < 40
- ê´€ì‹¬: 40 â‰¤ FAI < 60
- ì£¼ì˜: 60 â‰¤ FAI < 80
- ê²½ë³´: FAI â‰¥ 80`;
    }

    /**
     * ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° í¬í•¨)
     */
    buildUserPrompt(question, contextData, metadata) {
        let prompt = `ì‚¬ìš©ì ì§ˆë¬¸: ${question}\n\n`;

        if (contextData && contextData.length > 0) {
            prompt += `=== ê²€ìƒ‰ëœ ë°ì´í„° (${contextData.length}ê°œ) ===\n\n`;
            
            // ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
            contextData.slice(0, 10).forEach((row, index) => {
                prompt += `[ë°ì´í„° ${index + 1}]\n`;
                Object.keys(row).forEach(key => {
                    prompt += `  ${key}: ${row[key]}\n`;
                });
                prompt += `\n`;
            });

            if (contextData.length > 10) {
                prompt += `... ì™¸ ${contextData.length - 10}ê°œ ë” ìˆìŠµë‹ˆë‹¤.\n\n`;
            }
        } else {
            prompt += `âš ï¸ ê²€ìƒ‰ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ê³  ì•ˆë‚´í•˜ê³ , ê²€ìƒ‰ ì¡°ê±´ì„ í™•ì¸í•˜ë„ë¡ ì œì•ˆí•´ì£¼ì„¸ìš”.\n\n`;
        }

        if (metadata.targetColumns && metadata.targetColumns.length > 0) {
            prompt += `ì‚¬ìš©ìê°€ ê´€ì‹¬ ìˆëŠ” ì§€í‘œ: ${metadata.targetColumns.join(', ')}\n\n`;
        }

        if (metadata.queryConditions && Object.keys(metadata.queryConditions).length > 0) {
            prompt += `ê²€ìƒ‰ ì¡°ê±´:\n`;
            Object.entries(metadata.queryConditions).forEach(([key, value]) => {
                prompt += `  - ${key}: ${value}\n`;
            });
            prompt += `\n`;
        }

        // ê²½ê³  ì •ë³´ ì¶”ê°€
        if (metadata.alerts && metadata.alerts.length > 0) {
            prompt += `=== ê²½ê³  ì•Œë¦¼ (ì¤‘ìš”!) ===\n\n`;
            metadata.alerts.forEach((alert, index) => {
                prompt += `${index + 1}. ${alert.message}\n`;
                if (alert.manual) {
                    prompt += `   ğŸ’¡ ëŒ€ì‘ ë©”ë‰´ì–¼: ${alert.manual.title}\n`;
                }
            });
            prompt += `\n`;
            
            // ë©”ë‰´ì–¼ ë‚´ìš© ì¶”ê°€ (êµ¬ì²´ì ì¸ ì¡°ì¹˜ ë°©ë²•)
            if (metadata.manuals && metadata.manuals.length > 0) {
                prompt += `=== ëŒ€ì‘ ë©”ë‰´ì–¼ (êµ¬ì²´ì ì¸ ì¡°ì¹˜ ë°©ë²•) ===\n\n`;
                metadata.manuals.forEach((manual, index) => {
                    prompt += `[ë©”ë‰´ì–¼ ${index + 1}] ${manual.title}\n`;
                    prompt += `${manual.content}\n\n`;
                });
                prompt += `âš ï¸ ìœ„ ë©”ë‰´ì–¼ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ êµ¬ì²´ì ì¸ ì¡°ì¹˜ ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”. ë‹¨ìˆœíˆ "ë©”ë‰´ì–¼ì„ ì°¸ê³ í•˜ì„¸ìš”"ê°€ ì•„ë‹ˆë¼, ì‹¤ì œë¡œ ì–´ë–¤ ì¡°ì¹˜ë¥¼ í•´ì•¼ í•˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n\n`;
            }
            
            prompt += `âš ï¸ ìœ„ ê²½ê³  ì •ë³´ë¥¼ ë‹µë³€ì— ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•´ì£¼ì„¸ìš”. ê²½ê³ ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ê³ , ë©”ë‰´ì–¼ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ì¡°ì¹˜ ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”.\n\n`;
        }

        prompt += `ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. 
ë°ì´í„°ê°€ ìˆìœ¼ë©´ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ ì–¸ê¸‰í•˜ê³ , ìˆ˜ì§ˆ ë“±ê¸‰ì´ë‚˜ ì¡°ë¥˜ ê²½ë³´ ë‹¨ê³„ê°€ ìˆë‹¤ë©´ ê³„ì‚°í•˜ì—¬ í¬í•¨í•´ì£¼ì„¸ìš”.
ê²½ê³  ì •ë³´ê°€ ìˆìœ¼ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€ì— í¬í•¨ì‹œì¼œ ì£¼ì˜ë¥¼ í™˜ê¸°ì‹œì¼œì£¼ì„¸ìš”.
ì¶”ê°€ë¡œ í™•ì¸í•˜ë©´ ì¢‹ì„ ì •ë³´ë„ ì œì•ˆí•´ì£¼ì„¸ìš”.`;

        return prompt;
    }

    /**
     * OpenAI API í˜¸ì¶œ
     */
    async callOpenAI(systemPrompt, userPrompt) {
        const response = await fetch('https://api.openai.com/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.apiKey}`
            },
            body: JSON.stringify({
                model: this.model,
                messages: [
                    { role: 'system', content: systemPrompt },
                    { role: 'user', content: userPrompt }
                ],
                max_tokens: this.maxTokens,
                temperature: this.temperature
            })
        });

        if (!response.ok) {
            const error = await response.json().catch(() => ({ error: { message: 'Unknown error' } }));
            throw new Error(`OpenAI API ì˜¤ë¥˜: ${error.error?.message || response.statusText}`);
        }

        const data = await response.json();
        return data.choices[0].message.content.trim();
    }

    /**
     * Anthropic Claude API í˜¸ì¶œ
     */
    async callAnthropic(systemPrompt, userPrompt) {
        const response = await fetch('https://api.anthropic.com/v1/messages', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'x-api-key': this.apiKey,
                'anthropic-version': '2023-06-01'
            },
            body: JSON.stringify({
                model: this.model || 'claude-3-5-sonnet-20241022',
                max_tokens: this.maxTokens,
                temperature: this.temperature,
                system: systemPrompt,
                messages: [
                    { role: 'user', content: userPrompt }
                ]
            })
        });

        if (!response.ok) {
            const error = await response.json().catch(() => ({ error: { message: 'Unknown error' } }));
            throw new Error(`Anthropic API ì˜¤ë¥˜: ${error.error?.message || response.statusText}`);
        }

        const data = await response.json();
        return data.content[0].text.trim();
    }

    /**
     * Google Gemini API í˜¸ì¶œ (ë¬´ë£Œ í‹°ì–´ ì œê³µ)
     */
    async callGemini(systemPrompt, userPrompt) {
        const apiKey = this.apiKey;
        if (!apiKey) {
            throw new Error('Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. https://makersuite.google.com/app/apikey ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”.');
        }

        // GeminiëŠ” system promptë¥¼ user promptì— í¬í•¨
        const fullPrompt = `${systemPrompt}\n\n${userPrompt}`;

        const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${this.model || 'gemini-pro'}:generateContent?key=${apiKey}`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                contents: [{
                    parts: [{
                        text: fullPrompt
                    }]
                }],
                generationConfig: {
                    maxOutputTokens: this.maxTokens,
                    temperature: this.temperature
                }
            })
        });

        if (!response.ok) {
            const error = await response.json().catch(() => ({ error: { message: 'Unknown error' } }));
            throw new Error(`Gemini API ì˜¤ë¥˜: ${error.error?.message || response.statusText}`);
        }

        const data = await response.json();
        return data.candidates[0].content.parts[0].text.trim();
    }

    /**
     * Groq API í˜¸ì¶œ (ë¬´ë£Œ í‹°ì–´, ë§¤ìš° ë¹ ë¦„)
     */
    async callGroq(systemPrompt, userPrompt) {
        const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.apiKey}`
            },
            body: JSON.stringify({
                model: this.model || 'llama-3.1-8b-instant',
                messages: [
                    { role: 'system', content: systemPrompt },
                    { role: 'user', content: userPrompt }
                ],
                max_tokens: this.maxTokens,
                temperature: this.temperature
            })
        });

        if (!response.ok) {
            const error = await response.json().catch(() => ({ error: { message: 'Unknown error' } }));
            throw new Error(`Groq API ì˜¤ë¥˜: ${error.error?.message || response.statusText}`);
        }

        const data = await response.json();
        return data.choices[0].message.content.trim();
    }

    /**
     * Hugging Face Inference API í˜¸ì¶œ (ì¼ë¶€ ëª¨ë¸ ë¬´ë£Œ)
     */
    async callHuggingFace(systemPrompt, userPrompt) {
        const fullPrompt = `${systemPrompt}\n\n${userPrompt}`;
        
        const response = await fetch(`https://api-inference.huggingface.co/models/${this.model || 'mistralai/Mistral-7B-Instruct-v0.2'}`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': this.apiKey ? `Bearer ${this.apiKey}` : undefined
            },
            body: JSON.stringify({
                inputs: fullPrompt,
                parameters: {
                    max_new_tokens: this.maxTokens,
                    temperature: this.temperature,
                    return_full_text: false
                }
            })
        });

        if (!response.ok) {
            const error = await response.text();
            throw new Error(`Hugging Face API ì˜¤ë¥˜: ${error || response.statusText}`);
        }

        const data = await response.json();
        // Hugging Face ì‘ë‹µ í˜•ì‹ ì²˜ë¦¬
        if (Array.isArray(data) && data[0]?.generated_text) {
            return data[0].generated_text.trim();
        } else if (data.generated_text) {
            return data.generated_text.trim();
        } else if (typeof data === 'string') {
            return data.trim();
        }
        throw new Error('Hugging Face API ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜');
    }

    /**
     * OpenRouter API í˜¸ì¶œ (ë‹¤ì–‘í•œ ëª¨ë¸, ì¼ë¶€ ë¬´ë£Œ)
     */
    async callOpenRouter(systemPrompt, userPrompt) {
        const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.apiKey}`,
                'HTTP-Referer': window.location.origin, // ì„ íƒì‚¬í•­
                'X-Title': 'í™˜ê²½ ë°ì´í„° ì±—ë´‡' // ì„ íƒì‚¬í•­
            },
            body: JSON.stringify({
                model: this.model || 'openai/gpt-3.5-turbo',
                messages: [
                    { role: 'system', content: systemPrompt },
                    { role: 'user', content: userPrompt }
                ],
                max_tokens: this.maxTokens,
                temperature: this.temperature
            })
        });

        if (!response.ok) {
            const error = await response.json().catch(() => ({ error: { message: 'Unknown error' } }));
            throw new Error(`OpenRouter API ì˜¤ë¥˜: ${error.error?.message || response.statusText}`);
        }

        const data = await response.json();
        return data.choices[0].message.content.trim();
    }

    /**
     * Ollama API í˜¸ì¶œ (ë¡œì»¬ ì‹¤í–‰, ì™„ì „ ë¬´ë£Œ)
     */
    async callOllama(systemPrompt, userPrompt) {
        const baseURL = this.baseURL || 'http://localhost:11434';
        const fullPrompt = `${systemPrompt}\n\n${userPrompt}`;

        const response = await fetch(`${baseURL}/api/generate`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                model: this.model || 'llama2',
                prompt: fullPrompt,
                stream: false,
                options: {
                    num_predict: this.maxTokens,
                    temperature: this.temperature
                }
            })
        });

        if (!response.ok) {
            const error = await response.text();
            throw new Error(`Ollama API ì˜¤ë¥˜: ${error || response.statusText}`);
        }

        const data = await response.json();
        return data.response.trim();
    }

    /**
     * ì»¤ìŠ¤í…€ API í˜¸ì¶œ (ì˜ˆ: ìì²´ LLM ì„œë²„)
     */
    async callCustomAPI(systemPrompt, userPrompt) {
        if (!this.baseURL) {
            throw new Error('ì»¤ìŠ¤í…€ APIë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ baseURLì´ í•„ìš”í•©ë‹ˆë‹¤.');
        }

        const response = await fetch(this.baseURL, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': this.apiKey ? `Bearer ${this.apiKey}` : undefined
            },
            body: JSON.stringify({
                system_prompt: systemPrompt,
                user_prompt: userPrompt,
                model: this.model,
                max_tokens: this.maxTokens,
                temperature: this.temperature
            })
        });

        if (!response.ok) {
            throw new Error(`ì»¤ìŠ¤í…€ API ì˜¤ë¥˜: ${response.statusText}`);
        }

        const data = await response.json();
        // ì»¤ìŠ¤í…€ API ì‘ë‹µ í˜•ì‹ì— ë§ê²Œ ì¡°ì • í•„ìš”
        return data.response || data.text || data.content || JSON.stringify(data);
    }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í•¨ìˆ˜
export function createLLMClient(config) {
    return new LLMClient(config);
}

// ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ (config.jsì—ì„œ ì„¤ì • ë¡œë“œ)
export function getLLMClient() {
    const llmConfig = window.CONFIG?.LLM || {};
    
    if (!llmConfig.enabled) {
        return null; // LLMì´ ë¹„í™œì„±í™”ëœ ê²½ìš°
    }

    const provider = llmConfig.provider || 'groq';
    
    // providerë³„ ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
    const defaultModels = {
        'groq': 'llama-3.1-8b-instant',
        'gemini': 'gemini-pro',
        'huggingface': 'mistralai/Mistral-7B-Instruct-v0.2',
        'openrouter': 'openai/gpt-3.5-turbo',
        'ollama': 'llama2',
        'openai': 'gpt-4o-mini',
        'anthropic': 'claude-3-5-sonnet-20241022',
        'custom': 'custom-model'
    };

    return new LLMClient({
        provider: provider,
        apiKey: llmConfig.apiKey || '',
        model: llmConfig.model || defaultModels[provider] || 'gpt-4o-mini',
        baseURL: llmConfig.baseURL || (provider === 'ollama' ? 'http://localhost:11434' : null),
        maxTokens: llmConfig.maxTokens || 2000,
        temperature: llmConfig.temperature || 0.7
    });
}

export default LLMClient;

